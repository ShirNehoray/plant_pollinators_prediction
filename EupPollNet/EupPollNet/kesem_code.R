# ---- Predicting interactions across space with SVD ----
# this pipeline allows us to predict missing links using the softImpute algorithm, calculate evaluators, have some stats and correlate the evaluators with ecological data.
# here we focus on island scale, but there is a code for site-scale analysis and comparison between scales.
# stages are according to the pipeline figure (Fig. 1).
### code for publication ###
## ---- load libraries ----
library(tidyverse)
library(ggplot2)
library(dplyr)
library(reshape2)
library(ggpubr)
library(gridExtra)
library(grid)
library(scales)
library(cowplot)  # for get_legend()
library(corrplot)
library(patchwork)
library(vegan)
library(ggnewscale)
library(stringr)
library(softImpute)
library(ecodist)
library(rstatix)
library(ggrepel)
library(pROC)
library(PRROC)

## ---- themes ----
tme <-  theme(axis.text = element_text(size = 18, color = "black"),
              axis.title = element_text(size = 18, face = "bold"),
              panel.grid.major = element_blank(),
              panel.grid.minor = element_blank(),
              panel.border = element_rect(color = "black", fill = NA, linewidth = 1),
              axis.ticks = element_line(color = "black"))
theme_set(theme_bw())

## ---- parameters ----
Interaction_data <- read.csv("./csv/Interaction_edges.csv", encoding = "latin1", stringsAsFactors = FALSE)
one_study_interactions <- Interaction_data %>% 
  filter(Study_id == "1_Bartomeus")

emln_id <- "NA"
prop_ones_to_remove <- 0.2 # proportion of existing links to withhold
n_sim <- 50 # number of random link withholding and prediction iterations
set.seed(42) # the answer to everything

## ---- functions ----
# building matrices for combining matrices, calculating network size and density
build_interaction_matrix <- function(data, layers_to_filter) {
  # Step 1: Filter rows based on specified layers
  layers <- paste0("layer_", layers_to_filter)
  filtered_data <- subset(data, layer_from %in% layers)
  
  # Step 2: Aggregate weights for identical species pairs
  
  aggregated_data <- filtered_data %>%
    group_by(node_from, node_to) %>%
    summarise(weight = sum(weight), .groups = 'drop')
  
  # Step 3: Create the matrix with specific row and column species
  species_from <- unique(aggregated_data$node_from)  # Columns
  species_to <- unique(aggregated_data$node_to)      # Rows
  
  # Initialize an empty matrix
  interaction_matrix <- matrix(0, nrow = length(species_to), ncol = length(species_from),
                               dimnames = list(species_to, species_from))
  
  # Populate the matrix with aggregated weights
  for (i in 1:nrow(aggregated_data)) {
    row <- aggregated_data$node_to[i]    # Rows represent 'node_to' species
    col <- aggregated_data$node_from[i]  # Columns represent 'node_from' species
    interaction_matrix[row, col] <- aggregated_data$weight[i]
  }
  
  return(interaction_matrix)
}

# predict links using softImpute
implement_impute <- function(C, k, lambda) {
  # Apply softImpute
  
  fit <- softImpute(C, rank.max = k, lambda = lambda, type = "svd", maxit = 600)
  
  # Debias the fit to remove regularization effects
  # fit <- deBias(C, fit)
  
  # Reconstruct the matrix
  C_reconstructed <- softImpute::complete(C, fit)
  
  # Extract the reconstructed P matrix from C_reconstructed
  P_reconstructed <- C_reconstructed[rownames(P), colnames(P)]
  
  # Combine indices of removed ones and zeros
  if (is.null(dim(remove_indices))) { # handles when remove_indices has only one row
    test_indices <- rbind(
      data.frame(row = remove_indices["row"], col = remove_indices["col"], label = 1),
      data.frame(row = zeros_to_remove_indices[, "row"], col = zeros_to_remove_indices[, "col"], label = rep(0, nrow(zeros_to_remove_indices)))
    )
  } else {
    test_indices <- rbind(
      data.frame(row = remove_indices[, "row"], col = remove_indices[, "col"], label = rep(1, nrow(remove_indices))),
      data.frame(row = zeros_to_remove_indices[, "row"], col = zeros_to_remove_indices[, "col"], label = rep(0, nrow(zeros_to_remove_indices)))
    )
  }
  
  # Get the row and column names of the test links
  test_rows <- rownames(P)[test_indices$row]  # These are the "node_to"
  test_cols <- colnames(P)[test_indices$col]  # These are the "node_from"
  
  # Actual labels and predictions
  original_links <- P_original[cbind(test_rows, test_cols)]
  predicted_values <- P_reconstructed[cbind(test_rows, test_cols)]
  
  # Store the results with node information
  results <- data.frame(k = k,
                        lambda = lambda,
                        original_links = original_links,
                        predicted_values = predicted_values,
                        node_to = test_rows,
                        node_from = test_cols,
                        removed = 1 # mark these links as removed
  )
  
  # ---- (A) Enumerate ALL edges in P
  all_edges <- expand.grid(
    node_to = rownames(P),
    node_from = colnames(P),
    k = k,
    lambda = lambda,
    KEEP.OUT.ATTRS = FALSE,
    stringsAsFactors = FALSE
  )
  
  # Fill in the original link values from P_original
  all_edges$original_links <- mapply(
    function(r, c) P_original[r, c],
    all_edges$node_to,
    all_edges$node_from
  )
  
  # Helper data frame of removed edges
  removed_edges_idx <- data.frame(
    node_to = test_rows,
    node_from = test_cols,
    stringsAsFactors = FALSE
  )
  
  # ---- (B) Subset edges NOT removed
  not_removed <- all_edges[
    !paste(all_edges$node_to, all_edges$node_from) %in%
      paste(removed_edges_idx$node_to, removed_edges_idx$node_from), # all node pairs that are not in th removed list
  ]
  not_removed$removed <- 0
  not_removed$predicted_values <- NA
  
  # ---- (C) Combine removed + not removed
  not_removed$k <- k
  not_removed$lambda <- lambda
  
  list_results <- list(results=results, not_removed=not_removed)
  
  return(list_results)
}

# transforming raw predictions for binary evaluation
sigmoid <- function(x) {
  1 / (1 + exp(-x))
}

# calculate balanced per-species f1 score
# 2) A helper that, given one species’ data.frame, 
#    does one draw of a balanced F₁
compute_balanced_f1 <- function(df_sp) {
  # split positives / negatives
  pos <- df_sp %>% filter(original_binary == 1)
  neg <- df_sp %>% filter(original_binary == 0)
  
  n_pos <- nrow(pos)
  n_neg <- nrow(neg)
  
  # if either class is missing, we can’t compute F1
  if (n_pos == 0 || n_neg == 0) {
    return(NA_real_)
  }
  
  # sample size = the smaller of the two
  n <- min(n_pos, n_neg)
  
  # draw one balanced sample
  samp_pos <- pos %>% sample_n(n)
  samp_neg <- neg %>% sample_n(n)
  samp     <- bind_rows(samp_pos, samp_neg)
  
  # compute TP, FP, FN
  TP <- sum(samp$original_binary == 1 & samp$predicted_bin_sigm == 1)
  FP <- sum(samp$original_binary == 0 & samp$predicted_bin_sigm == 1)
  FN <- sum(samp$original_binary == 1 & samp$predicted_bin_sigm == 0)
  
  # precision, recall
  precision <- if ((TP + FP) > 0) TP / (TP + FP) else NA_real_
  recall    <- if ((TP + FN) > 0) TP / (TP + FN) else NA_real_
  
  # F1
  if (is.na(precision) || is.na(recall) || (precision + recall) == 0) {
    return(NA_real_)
  } else {
    return(2 * precision * recall / (precision + recall))
  }
}

# function to extract island names (removes "_site_X")
extract_island <- function(name) {
  gsub("_site_[12]", "", name)
}

# for MRM test
sym_average <- function(m) {
  mm <- m
  for(i in 1:nrow(mm)) for(j in 1:ncol(mm)) {
    if(i < j && !is.na(m[i,j]) && !is.na(m[j,i])) {
      avg       <- mean(c(m[i,j], m[j,i]))
      mm[i,j]   <- avg
      mm[j,i]   <- avg
    }
  }
  mm
}

# plotting

# plotting functions
# functions for diagonal and off-diagonal comparison
plot_boxplot <- function(data, metric, y_axis_label = "Balanced accuracy", 
                         stat_label_y = NULL, stat_size = 3) {
  
  # Calculate max value for y-axis and position for stat label
  max_y <- max(data[[metric]], na.rm = TRUE)
  label_y_position <- max_y * 0.98  # 95% of the maximum (a bit below the top)
  
  ggplot(data, aes(x = layer_comparison, y = .data[[metric]], fill = layer_comparison)) +
    geom_boxplot(notch = FALSE, alpha = 0.4, color = "black") +
    theme_minimal() +
    labs(y = y_axis_label) +   # y-axis title is set via the function argument
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1),
      legend.position = "none",
      axis.title.x = element_blank(),
      panel.border = element_rect(color = "black", fill = NA, linewidth = 1)
    ) +
    tme + 
    scale_fill_manual(values = custom_colors) +
    stat_compare_means(
      method = "t.test", label = "p.signif", hide.ns = FALSE, 
      comparisons = list(c("Diagonal", "Off-diagonals")),
      label.y = label_y_position * 1.1,  # Auto-set position
      size = stat_size
    ) +
    scale_y_continuous(limits = c(0.4, max_y * 1.1), labels = scales::number_format(accuracy = 0.1))  # Extend slightly above max
}

plot_hist <- function(data, metric, 
                      x_axis_label = "Balanced accuracy", 
                      y_axis_label = "Count") {
  ggplot(data, aes(x = .data[[metric]], fill = layer_comparison)) +
    geom_histogram(aes(y = ..count..), alpha = 0.4, color = "black", bins = 8, position = "dodge") +
    #geom_vline(xintercept = 0.5, linetype = "dashed", color = "black", linewidth = 1) +
    scale_x_continuous(labels = scales::number_format(accuracy = 0.1)) +
    theme_minimal() +
    labs(x = x_axis_label,
         y = y_axis_label,
         fill = "Layer comparison") +
    theme(
      text = element_text(size = 18),
      axis.text.x = element_text(hjust = 1),
      panel.border = element_rect(color = "black", fill = NA, linewidth = 1)
    ) +
    scale_fill_manual(values = custom_colors) + tme
}

plot_netsize <- function(data, evaluator = "f1_score",
                         facet_labels = NULL,
                         evaluator_label = NULL,
                         title_text = NULL) {
  
  evaluator_sym <- rlang::sym(evaluator)  # Treat evaluator as a column
  
  # Calculate correlations
  cor_table <- data %>%
    group_by(measure_type) %>%
    summarise(
      cor_value = cor(!!evaluator_sym, measure_value, use = "complete.obs", method = "pearson"),
      p_value   = cor.test(!!evaluator_sym, measure_value, method = "pearson")$p.value,
      .groups = "drop"
    )
  
  # Create annotation table
  cor_table_annot <- cor_table %>%
    mutate(
      r_fmt = formatC(cor_value, format = "f", digits = 2),
      p_fmt = ifelse(
        p_value < 0.001,
        formatC(p_value, format = "e", digits = 2),  # Scientific notation for very small p-values
        formatC(p_value, format = "f", digits = 3)   # Regular fixed format otherwise
      ),
      label_text = paste0("r = ", r_fmt, ", p = ", p_fmt)
    )
  
  
  # Build the plot
  plot <- ggplot(data, aes(x = measure_value, y = !!evaluator_sym)) +
    geom_point(color = "steelblue", alpha = 0.6, size = 2) +
    geom_smooth(method = "lm", se = FALSE, color = "salmon") +
    facet_wrap(
      ~ measure_type,
      scales   = "free_x",
      labeller = as_labeller(facet_labels)
    ) +
    scale_x_continuous(labels = scales::number_format(accuracy = 0.01)) +
    geom_text(
      data    = cor_table_annot,
      aes(label = label_text),
      x       = Inf,
      y       = Inf,
      hjust   = 1.1,
      vjust   = 1.2,
      size    = 3.2,
      inherit.aes = FALSE
    ) +
    labs(
      x = "Network feature",
      y = ifelse(is.null(evaluator_label), evaluator, evaluator_label),
      title = ifelse(is.null(title_text), paste(evaluator, "vs. network measures"), title_text)
    ) +
    theme_minimal() +
    tme +
    theme(
      panel.border = element_rect(color = "black", fill = NA, linewidth = 1),
      axis.ticks = element_line(color = "black"),
      strip.text = element_text(size = 12)
    )
  
  return(plot)
}

plot_f1_nnse_vs_size_free_both <- function(data) {
  # build correlation table
  cor_table <- data %>%
    group_by(evaluator, measure_type) %>%
    summarise(
      cor_value = cor(evaluator_value, measure_value, use = "complete.obs"),
      p_value   = cor.test(evaluator_value, measure_value, method = "pearson")$p.value,
      .groups   = "drop"
    ) %>%
    mutate(
      r_fmt      = formatC(cor_value, format = "f", digits = 2),
      p_fmt      = ifelse(
        p_value < 0.001,
        formatC(p_value, format = "e", digits = 2),
        formatC(p_value, format = "f", digits = 3)
      ),
      label_text = paste0("r = ", r_fmt, ", p = ", p_fmt)
    )
  
  ggplot(data, aes(x = measure_value, y = evaluator_value)) +
    geom_point(color = "steelblue", alpha = 0.6, size = 2) +
    geom_smooth(method = "lm", se = FALSE, color = "salmon") +
    
    facet_grid(
      rows   = vars(evaluator),
      cols   = vars(measure_type),
      scales = "free",     # ← free both x and y per facet
      labeller = labeller(
        evaluator    = c(f1_score = "F1 score", nnse = "NNSE"),
        measure_type = c(size_P  = "Size of matrix P",
                         size_C  = "Size of matrix C")
      ),
      switch = "y"
    ) +
    
    geom_text(
      data        = cor_table,
      aes(label    = label_text),
      x           = Inf, y    = Inf,
      hjust       = 1.1, vjust = 1.2,
      size        = 3.2,
      inherit.aes = FALSE
    ) +
    
    scale_x_continuous(
      name   = "Network size",
      expand = expansion(mult = c(0.05, 0.1))
    ) +
    
    scale_y_continuous(
      name   = NULL,                # remove y title
      expand = expansion(mult = c(0.05, 0.1))
    ) +
    
    #labs(title = "F1 score and RMSE vs. Size of matrices P and C") +
    
    theme_minimal() +
    theme(
      strip.placement    = "outside",
      strip.text.x       = element_text(size = 14),
      strip.text.y.left  = element_text(size = 14, face = "bold", angle = 90),
      panel.border       = element_rect(color = "black", fill = NA, linewidth = 1),
      axis.ticks         = element_line(color = "black"),
      strip.background   = element_blank()
    )
}

plot_f1_nnse_vs_density_free_both <- function(data) {
  # build correlation table
  cor_table <- data %>%
    group_by(evaluator, measure_type) %>%
    summarise(
      cor_value = cor(evaluator_value, measure_value, use = "complete.obs"),
      p_value   = cor.test(evaluator_value, measure_value, method = "pearson")$p.value,
      .groups   = "drop"
    ) %>%
    mutate(
      r_fmt      = formatC(cor_value, format = "f", digits = 2),
      p_fmt      = ifelse(
        p_value < 0.001,
        formatC(p_value, format = "e", digits = 2),
        formatC(p_value, format = "f", digits = 3)
      ),
      label_text = paste0("r = ", r_fmt, ", p = ", p_fmt)
    )
  
  ggplot(data, aes(x = measure_value, y = evaluator_value)) +
    geom_point(color = "steelblue", alpha = 0.6, size = 2) +
    geom_smooth(method = "lm", se = FALSE, color = "salmon") +
    
    facet_grid(
      rows   = vars(evaluator),
      cols   = vars(measure_type),
      scales = "free",     # ← free both x and y per facet
      labeller = labeller(
        evaluator    = c(f1_score = "F1 score", nnse = "NNSE"),
        measure_type = c(density_P  = "Connectance of matrix P",
                         density_C  = "Connectance of matrix C")
      ),
      switch = "y"
    ) +
    
    geom_text(
      data        = cor_table,
      aes(label    = label_text),
      x           = Inf, y    = Inf,
      hjust       = 1.1, vjust = 1.2,
      size        = 3.2,
      inherit.aes = FALSE
    ) +
    
    scale_x_continuous(
      name   = "Network connectance",
      breaks = scales::breaks_width(0.02),       # 0.02 between ticks
      labels = scales::label_number(accuracy = 0.01),
      expand = expansion(mult = c(0.05, 0.05))
    ) +
    
    scale_y_continuous(
      name   = NULL,                # remove y title
      expand = expansion(mult = c(0.05, 0.1))
    ) +
    
    #labs(title = "F1 score and RMSE vs. Size of matrices P and C") +
    
    theme_minimal() +
    theme(
      strip.placement    = "outside",
      strip.text.x       = element_text(size = 12),
      strip.text.y.left  = element_text(size = 14, face = "bold", angle = 90),
      panel.border       = element_rect(color = "black", fill = NA, linewidth = 1),
      axis.ticks         = element_line(color = "black"),
      strip.background   = element_blank(),
      panel.spacing.x    = unit(0.7, "cm")
      
    )
}

make_facet_scatter_plot <- function(data,
                                    evaluator = "f1_score", 
                                    pivot_cols = c("jaccard_pollinators", "jaccard_plants", "jaccard_edges"),
                                    names_to = "jaccard_type", 
                                    values_to = "jaccard_value",
                                    x_lab = "Jaccard similarity",
                                    y_lab = "F1 score",
                                    plot_title = NULL,
                                    facet_scales = "free_x") {
  
  # Reshape data from wide to long format for the specified pivot columns
  df_long <- data %>% 
    pivot_longer(cols = all_of(pivot_cols), 
                 names_to = names_to, 
                 values_to = values_to)
  
  # For each facet (jaccard_type), compute correlation between the evaluator and jaccard_value
  cor_table <- df_long %>%
    group_by(!!sym(names_to)) %>%
    summarise(
      cor_value = cor(.data[[evaluator]], .data[[values_to]], use = "complete.obs", method = "pearson"),
      p_value   = cor.test(.data[[evaluator]], .data[[values_to]], method = "pearson")$p.value
    ) %>%
    ungroup()
  
  # Create annotations with formatted correlation coefficients and p-values
  cor_table_annot <- cor_table %>%
    mutate(
      r_fmt = formatC(cor_value, format = "f", digits = 2),
      p_fmt = ifelse(
        p_value < 0.001,
        formatC(p_value, format = "e", digits = 2),  # Scientific notation
        formatC(p_value, format = "f", digits = 3)   # Otherwise
      ),
      label_text = paste0("r = ", r_fmt, ", p = ", p_fmt)
    )
  
  # Facet labels (renaming)
  facet_labels <- c(
    jaccard_edges = "Interaction overlap",
    jaccard_plants = "Plants overlap",
    jaccard_pollinators = "Pollinators overlap"
  )
  
  # Construct the faceted scatter plot
  plot <- ggplot(df_long, aes_string(x = values_to, y = evaluator)) +
    geom_point(color = "steelblue", alpha = 0.6, size = 2) +
    geom_smooth(method = "lm", se = FALSE, color = "thistle") +
    facet_wrap(as.formula(paste("~", names_to)), 
               scales = facet_scales,
               labeller = as_labeller(facet_labels)) +
    scale_x_continuous(labels = number_format(accuracy = 0.02)) +
    # Place the correlation annotation in the upper-right corner of each facet
    geom_text(data = cor_table_annot,
              aes(label = label_text),
              x = Inf,
              y = Inf,
              hjust = 1.1,
              vjust = 1.2,
              size = 3.2,
              color = "black") +
    labs(x = x_lab, y = y_lab, title = plot_title) +
    theme_minimal() +
    tme +
    theme(
      strip.text = element_text(size = 12),  # <-- Facet titles larger and bold
      panel.border = element_rect(color = "black", fill = NA, size = 1),
      axis.ticks = element_line(color = "black"),
      axis.text.x     = element_text(size = 9)                          )
  
  return(plot)
}

make_simple_correlation_plot <- function(data,
                                         x_var,
                                         evaluator,
                                         x_lab = NULL,
                                         y_lab = NULL,
                                         plot_title,
                                         point_color,
                                         trend_color = "steelblue",
                                         x_axis_blank = FALSE,
                                         y_axis_blank = FALSE) {
  
  # Perform correlation test
  correlation <- cor.test(data[[evaluator]], data[[x_var]], use = "complete.obs", method = "pearson")
  
  # Extract correlation coefficient and p-value
  r_value <- round(correlation$estimate, 2)
  p_value <- ifelse(
    correlation$p.value < 0.001,
    formatC(correlation$p.value, format = "e", digits = 2),  # scientific for very small
    formatC(correlation$p.value, format = "f", digits = 3)   # fixed format otherwise
  )  
  
  label_text <- paste0("r = ", r_value, ", p = ", p_value)
  
  # Build the plot
  p <- ggplot(data, aes_string(x = x_var, y = evaluator)) +
    geom_point(color = point_color, alpha = 0.6, size = 2) +
    geom_smooth(method = "lm", se = FALSE, color = trend_color) +
    labs(
      x = x_lab,
      y = y_lab,
      title = plot_title
    ) +
    tme +
    annotate("text",
             x = Inf, y = Inf,
             hjust = 1.1, vjust = 1.2,
             label = label_text,
             size = 4,
             color = "black")
  
  # Optionally remove axis titles
  if (y_axis_blank) {
    p <- p + theme(axis.title.y = element_blank(),
                   axis.text.y = element_blank())
  }
  if (x_axis_blank) {
    p <- p + theme(axis.title.x = element_blank())
  }
  
  return(p)
}

# Final master function to make the full double plot
make_full_correlation_plot <- function(data,
                                       evaluator = "f1_score",
                                       pollinator_x = "avg_sorensen_pollinators",
                                       plant_x = "avg_sorensen_plants",
                                       shared_x_lab = "Mean Sorensen similarity",
                                       shared_y_lab = NULL) {
  
  if (is.null(shared_y_lab)) {
    # If user doesn't specify left y-axis label, use the evaluator name nicely formatted
    shared_y_lab <- gsub("_", " ", evaluator)
    shared_y_lab <- str_to_title(shared_y_lab)
  }
  
  # Create plots (with no x labels)
  pollinator_plot <- make_simple_correlation_plot(
    data = data,
    x_var = pollinator_x,
    evaluator = evaluator,
    x_lab = NULL,  # No individual x-label
    y_lab = NULL,  # No individual y-label
    plot_title = "Pollinators",
    point_color = "thistle",
    trend_color = "steelblue",
    x_axis_blank = TRUE,
    y_axis_blank = TRUE
  )
  
  plant_plot <- make_simple_correlation_plot(
    data = data,
    x_var = plant_x,
    evaluator = evaluator,
    x_lab = NULL,  # No individual x-label
    y_lab = NULL,  # No individual y-label
    plot_title = "Plants",
    point_color = "darkseagreen3",
    trend_color = "steelblue",
    x_axis_blank = TRUE,
    y_axis_blank = FALSE
  )
  
  plant_plot <- plant_plot + theme(plot.margin = ggplot2::margin(4, 10, 4, 10))
  pollinator_plot <- pollinator_plot + theme(plot.margin = ggplot2::margin(4, 10, 4, 10))
  
  
  # Arrange plots side by side
  # plots_side_by_side <- arrangeGrob(
  #   plant_plot, pollinator_plot,
  #   ncol = 2
  # )
  # Arrange plots side by side with equal widths
  plots_side_by_side <- arrangeGrob(
    plant_plot, pollinator_plot,
    ncol = 2,
    widths = unit.c(unit(1.13, "null"), unit(1, "null"))  # Equal widths
  )
  
  
  # Add shared axis labels
  final_plot <- grid.arrange(
    plots_side_by_side,
    left = textGrob(shared_y_lab, rot = 90, gp = gpar(fontsize = 13, fontface = "bold")),
    bottom = textGrob(shared_x_lab, gp = gpar(fontsize = 13, fontface = "bold"))
  )
  
  return(final_plot)
}

# combine plots
combine_plots <- function(p1, p2,
                          bottom_label = "Overall degree",
                          left_label = "Number of predicted, \nnon-observed links",
                          plot_margin = c(0.5, 0.5, 1, 0.3),
                          label_fontsize = 16,
                          label_fontface = "bold",
                          widths_subplots = c(1, 1),
                          final_widths = c(2, 0.3)) {
  
  # Load required packages
  require(ggplot2)
  require(gridExtra)
  require(grid)
  
  # Adjust individual plots
  p1_mod <- p1 +
    theme(legend.position = "none",
          axis.title = element_blank(),
          plot.margin = unit(plot_margin, "cm"))
  
  p2_mod <- p2 +
    theme(legend.position = "none",
          axis.title = element_blank(),
          plot.margin = unit(plot_margin, "cm"))
  
  # Arrange the two plots side-by-side
  combined_plots <- arrangeGrob(p1_mod, p2_mod, 
                                ncol = 2, 
                                widths = widths_subplots)
  
  # Add axis labels using arrangeGrob (the bottom and left text grobs)
  combined_with_axes <- arrangeGrob(
    combined_plots,
    bottom = textGrob(bottom_label, 
                      gp = gpar(fontsize = label_fontsize, fontface = label_fontface), 
                      vjust = -1.5),
    left   = textGrob(left_label, 
                      rot = 90, 
                      gp = gpar(fontsize = label_fontsize, fontface = label_fontface))
  )
  
  return(combined_with_axes)
}

make_cor_plot <- function(data, evaluator, 
                          distance_col = "distance_km", 
                          x_lab = "Geographic distance (km)",
                          y_lab = NULL,
                          extra_theme = NULL) {
  # Use evaluator as y_lab if no alternative is provided
  if (is.null(y_lab)) {
    y_lab <- evaluator
  }
  
  # Compute correlation between evaluator and distance
  correlation <- cor.test(data[[evaluator]], data[[distance_col]], 
                          use = "complete.obs", method = "pearson")
  r_value <- round(correlation$estimate, 2)
  p_value <- ifelse(
    correlation$p.value < 0.001,
    formatC(correlation$p.value, format = "e", digits = 2),  # scientific for very small
    formatC(correlation$p.value, format = "f", digits = 3)   # fixed format otherwise
  ) 
  label_text <- paste0("r = ", r_value, ", p = ", p_value)
  
  # Create plot with label in the upper right corner using Inf coordinates
  plot <- ggplot(data, aes_string(x = distance_col, y = evaluator)) +
    geom_point(color = "salmon2", size = 2) +
    geom_smooth(method = "lm", se = FALSE, color = "steelblue2") +
    labs(x = x_lab, y = y_lab) +
    # The following places the label at the upper right of the plot area
    annotate("text", x = Inf, y = Inf, label = label_text,
             hjust = 1.1, vjust = 1.1, size = 3.5, color = "black")
  
  # Optionally add additional theme modifications
  if (!is.null(extra_theme)) {
    plot <- plot + extra_theme
  }
  
  return(plot)
}

combine_two_plots <- function(p1, p2,
                              x_axis_label = "Geographic distance (km)",
                              y_axis_label = "F1 score",
                              p1_title = "Site scale",
                              p2_title = "Island scale",
                              margins = unit(c(0.5, 0.5, 1, 0.3), "cm"),
                              axis_title_fontsize = 14,
                              axis_title_fontface = "bold") {
  
  # Adjust first plot
  p1 <- p1 +
    ggtitle(p1_title) +
    theme(
      legend.position = "none",
      axis.title = element_blank(),
      plot.margin = margins
    )
  
  # Adjust second plot
  p2 <- p2 +
    ggtitle(p2_title) +
    theme(
      legend.position = "none",
      axis.title = element_blank(),
      axis.text.y = element_blank(),
      plot.margin = margins
    )
  
  # Combine p1 and p2 side by side
  combined_plots <- arrangeGrob(
    p1, p2,
    ncol = 2,
    widths = c(1.1, 1)
  )
  
  # Add global x and y axis labels
  combined_with_axes <- arrangeGrob(
    combined_plots,
    bottom = textGrob(
      x_axis_label,
      gp = gpar(fontsize = axis_title_fontsize, fontface = axis_title_fontface),
      vjust = -1.5
    ),
    left = textGrob(
      y_axis_label,
      rot = 90,
      gp = gpar(fontsize = axis_title_fontsize, fontface = axis_title_fontface)
    )
  )
  
  # Final arrangement
  final_plot <- grid.arrange(
    combined_with_axes,
    ncol = 2,
    widths = c(2, 0.01)
  )
  
  return(final_plot)
}

## ---- 1. prediction ----
### ---- load matrices ----
A_l <-  one_study_interactions


# Create lookup table for network IDs
layer_names <- unique(A_l$layer_from)
layer_map <- data.frame(
  original = layer_names,
  new_id = paste0("layer_", seq_along(layer_names))
)

# Apply the mapping
aggregated_df <- A_l %>%
  left_join(layer_map, by = c("layer_from" = "original")) %>%
  mutate(layer_from = new_id,
         layer_to   = new_id) %>%
  select(layer_from, node_from, layer_to, node_to, weight, type)


# # Aggregate data
# aggregated_df <- A_l %>%
#   group_by(aggregated_layer, node_from, node_to, type) %>%
#   summarise(weight = sum(weight), .groups = "drop") %>%
#   mutate(layer_from = aggregated_layer, layer_to = aggregated_layer) %>%
#   select(layer_from, node_from, layer_to, node_to, weight, type)

# set new layer names using the old ones
# aggregated_df <- aggregated_df %>%
#   separate_wider_delim(layer_from, delim = "_", names = c("t", "l1", "l2"), cols_remove = FALSE) %>%
#   mutate(island_id = paste0("layer_", as.numeric(l2)/2))  %>%
#   mutate(layer_from = island_id, layer_to = island_id)%>%
#   select(layer_from, node_from, layer_to, node_to, weight, type)


# save aggregated network to a file
# write.csv(aggregated_df, file = "prediction_pipeline_for_publication/results/network_island_scale.csv", row.names = FALSE)

# Total number of layers
num_layers <- length(unique(aggregated_df$layer_from))

results_file <- "predictions_island_scale.rds"

# read the prediction data if you already have it, and if not generate predictions

# Initialize a data frame to store combined results for all layer combinations
combined_results <- data.frame()

if (file.exists(results_file)) {
  print("Existing results file found — reading the file and proceeding to analysis")
  
  combined_results <- readRDS(results_file)
  print("finished loading prediction results")
  
} else { # or alternatively run the prediction pipeline
  # Loop through all combinations of layers_to_train and layer_to_predict
  for (layers_to_train in 1:num_layers) {
    for (layer_to_predict in 1:num_layers) {
      print(paste("** from:", layers_to_train, " to:", layer_to_predict, "**"))
      
      # Build the aggregated matrix A for training
      A <- build_interaction_matrix(data = aggregated_df, layers_to_filter = layers_to_train)
      
      # Build the layer to predict matrix P
      P <- build_interaction_matrix(data = aggregated_df, layers_to_filter = layer_to_predict)
      
      node_to <- rownames(P) # for the results
      node_from <- colnames(P)
      
      ### ---- a. withhold links in P ----
      # map out the 0s and 1s in P
      num_1_to_remove <- floor(sum(P>0, na.rm = T)*prop_ones_to_remove)  # Number of links to remove
      ones_in_P <- which(P > 0, arr.ind = TRUE)
      
      num_0_to_remove <- num_1_to_remove
      prop_0_removed <- num_0_to_remove / sum(P == 0, na.rm = T)
      zeros_in_P <- which(P == 0, arr.ind = TRUE)
      
      # debug print
      print(paste("1 remove:", num_1_to_remove))
      print(paste("all 1   :", nrow(ones_in_P)))
      print(paste("0s to remove:", num_0_to_remove))
      print(paste("all zeros   :", nrow(zeros_in_P)))
      print(paste("prop of zeros removed   : ", prop_0_removed))
      
      # Randomly select zeros to withhold - bootstrapping
      bootstrapping_results <- NULL
      P_original <- P # save it for later
      
      for (i in 1:n_sim) {
        # remove 1s
        remove_indices <- ones_in_P[sample(1:nrow(ones_in_P), num_1_to_remove), ]
        P[remove_indices] <- NA  # Set removed links to NA
        
        # sample 0s
        zeros_to_remove_indices <- zeros_in_P[sample(1:nrow(zeros_in_P), num_0_to_remove), ]
        P[zeros_to_remove_indices] <- NA
        
        ### ---- creating a combined matrix C ----
        # Combine A and P into a single matrix C with NAs representing missing data
        all_row_ids <- unique(c(rownames(A), rownames(P)))
        all_col_ids <- unique(c(colnames(A), colnames(P)))
        C <- matrix(0, nrow = length(all_row_ids), ncol = length(all_col_ids),
                    dimnames = list(all_row_ids, all_col_ids))
        
        # Place A into C
        C[rownames(A), colnames(A)] <- A
        
        # Place P into C
        # Ensure that existing entries are not overwritten; sum overlapping entries
        C[rownames(P), colnames(P)] <- ifelse(is.na(C[rownames(P), colnames(P)]), 
                                              NA, 
                                              C[rownames(P), colnames(P)] + P[rownames(P), colnames(P)])
        
        
        # Apply biScale to center matrices
        C <- biScale(C, row.center=TRUE, col.center=TRUE, row.scale=FALSE, col.scale=FALSE)
        
        sum(is.na(C))
        
        ### ---- b. + d. prediction with SVD and apply for all network combinations ----
        k_values <- c(2, 5, 10)
        lam0 <- lambda0(C)
        lambda_values <- c(1, 5, 50, 100, lam0)
        
        # Initialize variables to store the best results
        results <- data.frame(k = integer(),
                              lambda = numeric(),
                              original_links = numeric(),
                              predicted_values = numeric(),
                              input_lambda = numeric())
        not_removed_all <- NULL
        
        # Loop over all combinations of k and lambda
        for (k in k_values) {
          for (lambda in lambda_values) {
            # imputation
            r <- implement_impute(C, k, lambda)
            r$results$input_lambda <- lambda
            r$not_removed$input_lambda <- lambda
            results <- rbind(results, r$results)
            not_removed_all <- rbind(not_removed_all, r$not_removed)
          }
        }
        
        ### ---- save results for current k/lambda combination ----
        # After finishing the k/lambda loops, append the 'results' to 'combined_results'
        # ---- (D) Append to combined_results
        complete_edges_all <- rbind(results, not_removed_all)
        complete_edges_all$itr <- i
        bootstrapping_results <- rbind(bootstrapping_results, complete_edges_all)
        
        # reset P
        P <- P_original
      }
      
      combined_results <- rbind(
        combined_results,
        cbind(
          data.frame(
            emln_id = emln_id,
            train_layer = layers_to_train,
            test_layer = layer_to_predict,
            prop_ones_removed = prop_ones_to_remove,
            amount_of_removed_1 = num_1_to_remove,
            amount_of_removed_0 = num_0_to_remove,
            prop_0_removed = prop_0_removed
          ),
          bootstrapping_results
        )
      )
    }
  }
  # combined_results includes predictions for all combinations of islands, 50 iterations of links withholding and prediction for each combination
  # save the results
  saveRDS(combined_results, file = results_file)
} 

# after reading or producing the results, filter these (important!):
combined_results <- combined_results %>% 
  filter(k == 2) %>% 
  filter(!(input_lambda %in%  c(1, 5, 50, 100)))

## ---- 2. analysis ----
summary(combined_results)

# convert negatives to zeros
df <- combined_results %>%
  mutate(predicted_values = if_else(predicted_values < 0, 0, predicted_values))

# know thy network - what species do we have in the system?
plant_species <- unique(df$node_from)           # get unique names
pollinator_species <- unique(df$node_to)
length(plant_species)
length(pollinator_species)

### ---- c. evaluation ----
### ---- Fig. S8: selecting optimal threshold ----
# select the threshold for classifying links as 1s or 0s based on balance between f1 and balanced accuracy

# 0) set an array of thresholds
thresholds <- seq(0, 1, by = 0.1)

# 1) filter & prep
df_prepped <- df %>%
  filter(removed == 1) %>%
  mutate(
    predicted_prob   = sigmoid(predicted_values),
    original_binary  = if_else(original_links > 0, 1, 0)
  )

# 2) expand to one row per threshold
df_thresh <- df_prepped %>%
  tidyr::expand_grid(threshold = thresholds) %>%  # <-- switch here
  mutate(
    predicted_bin = if_else(predicted_prob > threshold, 1, 0) #binary
  ) %>%
  group_by(emln_id, train_layer, test_layer, itr, threshold) %>%
  summarise(
    TP = sum(original_binary == 1 & predicted_bin == 1),
    FN = sum(original_binary == 1 & predicted_bin == 0),
    TN = sum(original_binary == 0 & predicted_bin == 0),
    FP = sum(original_binary == 0 & predicted_bin == 1),
    specificity      = TN / (TN + FP), #True negative rate
    precision        = TP / (TP + FP), #Positive Predictive Value
    recall           = TP / (TP + FN), #True Positive Rate
    f1_score         = 2 * (precision * recall) / (precision + recall),  #mean of precision and recall
    balanced_accuracy= (recall + specificity) / 2,
    mcc = (TP * TN - FP * FN) /
      sqrt((TP + FP)*(TP + FN)*(TN + FP)*(TN + FN)),  #Matthews Correlation Coefficient (-1 to 1)
    mse  = mean((predicted_values - original_links)^2, na.rm = TRUE),
    rmse = sqrt(mse)#,
    #.groups = "drop"
  ) %>%
  ungroup() %>%
  group_by(emln_id, train_layer, test_layer, threshold) %>%
  summarise(
    TP = mean(TP, na.rm = TRUE),
    FN = mean(FN, na.rm = TRUE),
    TN = mean(TN, na.rm = TRUE),
    FP = mean(FP, na.rm = TRUE),
    specificity = mean(specificity, na.rm = TRUE),
    precision = mean(precision, na.rm = TRUE),
    recall = mean(recall, na.rm = TRUE),
    f1_score = mean(f1_score, na.rm = TRUE),
    balanced_accuracy = mean(balanced_accuracy, na.rm = TRUE),
    mcc = mean(mcc, na.rm = TRUE),
    mse = mean(mse, na.rm = TRUE),
    rmse = mean(rmse, na.rm = TRUE)
  ) %>%
  ungroup() 


write.csv(df_thresh, "df_thresh_1_Bartomeus.csv")

# 3) average across emln_id/layer combos and pivot long
df_avg <- df_thresh %>%
  group_by(threshold) %>%
  summarise(across(
    c(specificity, precision, recall,
      f1_score, balanced_accuracy, mcc),
    mean, na.rm = TRUE
  )) %>%
  pivot_longer(-threshold,
               names_to  = "metric",
               values_to = "value")

df_avg_plot <- df_avg %>% mutate(metric = recode(metric,
                                                 specificity       = "Specificity",
                                                 precision         = "Precision",
                                                 recall            = "Recall",
                                                 f1_score          = "F1 score",
                                                 balanced_accuracy = "Balanced accuracy",
                                                 mcc               = "MCC"
))

# 4) plot
optimal_threshold <- ggplot(df_avg_plot, aes(threshold, value, color = metric)) +
  geom_line(size = 1) +
  labs(
    x     = "Probability threshold",
    y     = "Average metric",
    color = "Metric"
  ) +
  scale_color_brewer(palette = "Pastel2") +
  tme

# pdf(
#   file   = "results/paper_figs/optimal_threshold.pdf",
#   width  = 5,    # inches
#   height = 4,
#   family = "Helvetica"   # or another installed font
# )
# print(optimal_threshold)
# dev.off()     # close the file

df_eval <- df %>%
  filter(removed == 1) %>%
  mutate(
    predicted_prob  = sigmoid(predicted_values),
    original_binary = if_else(original_links > 0, 1L, 0L)
  ) %>%
  group_by(emln_id, train_layer, test_layer, itr) %>%
  summarise(
    # ROC-AUC (coerce to numeric!)
    auc_roc = tryCatch({
      roc_obj <- roc(response = original_binary,
                     predictor = predicted_prob,
                     quiet = TRUE, na.rm = TRUE,
                     levels = c(0,1), direction = "<")
      as.numeric(auc(roc_obj))   # <-- important
    }, error = function(e) NA_real_),
    
    # PR-AUC (guard against all-one-class cases)
    auc_pr = tryCatch({
      pos <- predicted_prob[original_binary == 1]
      neg <- predicted_prob[original_binary == 0]
      if (length(pos) == 0 || length(neg) == 0) return(NA_real_)
      pr_obj <- pr.curve(scores.class0 = pos, scores.class1 = neg, curve = FALSE)
      pr_obj$auc.integral
    }, error = function(e) NA_real_)
  ) %>%
  ungroup()

df_eval_summary <- df_eval %>%
  group_by(emln_id, train_layer, test_layer) %>%
  summarise(
    auc_roc_mean = mean(auc_roc, na.rm = TRUE),
    auc_roc_sd   = sd(auc_roc,   na.rm = TRUE),
    auc_pr_mean  = mean(auc_pr,  na.rm = TRUE),
    auc_pr_sd    = sd(auc_pr,    na.rm = TRUE),
    .groups = "drop"
  )


# 1) pivot to wide so F1 and balanced_accuracy are columns
# we aim to find the optimal balance between ba and f1
df_wide <- df_avg %>%
  pivot_wider(names_from = metric, values_from = value) %>%
  arrange(threshold)

# 2) discrete approx: minimize abs difference
best_discrete <- df_wide %>%
  mutate(absdiff = abs(f1_score - balanced_accuracy)) %>%
  slice_min(absdiff, n = 1)

# results:
best_discrete_threshold <- best_discrete$threshold
best_discrete_threshold

df_removed <- df %>%
  filter(removed == 1) %>% 
  mutate(predicted_prob_sigm = sigmoid(predicted_values)) %>%  # convert the predicted values to probability values in the interval (0, 1) using the logistic function
  mutate(predicted_bin_sigm = if_else(predicted_prob_sigm > best_discrete_threshold, 1, 0)) %>% 
  mutate(original_binary = if_else(original_links > 0, 1, 0))


# ---- link taxonomy ----

## ---- full analysis ----
# run the flagging logic on the full data set (including non-withheld interactions)
df_self <- df %>%
  filter(train_layer == test_layer) %>% 
  mutate(predicted_prob_sigm = sigmoid(predicted_values)) %>%  # convert the predicted values to probability values in the interval (0, 1) using the logistic function
  mutate(predicted_bin_sigm = if_else(predicted_prob_sigm > best_discrete_threshold, 1, 0)) %>% 
  mutate(original_binary = if_else(original_links > 0, 1, 0))

df_self <- df_self %>%
  mutate(
    original_binary    = as.integer(original_binary),
    interaction_id     = paste0(node_from, " -> ", node_to)
  )

# Observation count per interaction across all islands (networks)- sum the binary
obs_counts <- df_self %>%
  distinct(test_layer, interaction_id, original_binary) %>%
  group_by(interaction_id) %>%
  summarise(n_obs_total = sum(original_binary == 1, na.rm = TRUE), .groups = "drop")

# Join those flags to df_removed (withholding data)
# restrict to self-predictions:
df_removed <- df_removed %>% filter(test_layer == train_layer)

df_removed_flagged <- df_removed %>%
  mutate(
    original_binary    = as.integer(original_binary),
    predicted_bin_sigm = as.integer(predicted_bin_sigm),
    interaction_id     = paste0(node_from, " -> ", node_to)
  ) %>%
  left_join(obs_counts, by = "interaction_id") %>%
  mutate(
    is_all_zero   = n_obs_total == 0,
    is_unique     = n_obs_total == 1,
    is_shared     = n_obs_total >= 2,
    obs_elsewhere = (n_obs_total - (original_binary == 1)) >= 1  # observed in ≥1 other island
  )

# write.csv(df_removed_flagged, "df_removed_flagged_1_Bartomeus.csv")
#table for box plot
# produce results table


# produce results table
final_table_by_iter <- df_removed_flagged %>%
  group_by(itr) %>%
  summarise(
    TP = sum(original_binary == 1 & predicted_bin_sigm == 1),
    FP = sum(original_binary == 0 & predicted_bin_sigm == 1),
    TN = sum(original_binary == 0 & predicted_bin_sigm == 0),
    FN = sum(original_binary == 1 & predicted_bin_sigm == 0),
    
    locally_unique_links = sum(is_unique  & original_binary == 1 & predicted_bin_sigm == 1),
    unsupported_links    = sum(is_unique  & original_binary == 1 & predicted_bin_sigm == 0),
    
    confirmed_links      = sum(is_shared  & original_binary == 1 & predicted_bin_sigm == 1),
    cryptic_links        = sum(is_shared  & original_binary == 1 & predicted_bin_sigm == 0),
    
    likely_forbidden     = sum(is_all_zero & original_binary == 0 & predicted_bin_sigm == 0),
    spurious_links       = sum(is_all_zero & original_binary == 0 & predicted_bin_sigm == 1),
    
    feasible_links       = sum(original_binary == 0 & obs_elsewhere & predicted_bin_sigm == 0),
    possibly_missing_links = sum(original_binary == 0 & obs_elsewhere & predicted_bin_sigm == 1),
    .groups = "drop"
  ) %>%
  tidyr::pivot_longer(-itr, names_to = "Category", values_to = "Count") %>%
  arrange(itr, Category)

# summary table
final_table_summary <- final_table_by_iter %>%
  group_by(Category) %>%
  summarise(
    mean = mean(Count), sd = sd(Count),
    min = min(Count), max = max(Count),
    .groups = "drop"
  ) %>%
  arrange(Category)

## ---- plot ----
# install.packages(c("ggplot2","ggalluvial","dplyr","tidyr","stringr","scales"))
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(ggalluvial)
library(scales)

# INPUT: final_table_summary with columns: Category, mean
# If needed, reconstruct from final_table_by_iter:
# final_table_summary <- final_table_by_iter %>% group_by(Category) %>%
#   summarise(mean = mean(Count), .groups = "drop")

# 1) Named vector of means (missing -> 0)
cat_means <- final_table_summary %>%
  transmute(Category, mean = coalesce(mean, 0)) %>%
  tibble::deframe()

get_mean <- function(nm) if (nm %in% names(cat_means)) unname(cat_means[[nm]]) else 0

# 2) Build flows: L1 (confusion) → L2 (validation) → L3 (subcategory)
flows <- tibble::tribble(
  ~L1, ~L2,                   ~L3,                     ~value,
  "TP","Validated elsewhere", "confirmed_links",        get_mean("confirmed_links"),
  "TP","Not validated",       "locally_unique_links",   get_mean("locally_unique_links"),
  "FN","Validated elsewhere", "cryptic_links",          get_mean("cryptic_links"),
  "FN","Not validated",       "unsupported_links",      get_mean("unsupported_links"),
  "FP","Validated elsewhere", "possibly_missing_links", get_mean("possibly_missing_links"),
  "FP","Not validated",       "spurious_links",         get_mean("spurious_links"),
  "TN","Validated elsewhere", "feasible_links",         get_mean("feasible_links"),
  "TN","Not validated",       "likely_forbidden",       get_mean("likely_forbidden")
) %>%
  mutate(
    total_confusion = get_mean("TP")+get_mean("FP")+get_mean("TN")+get_mean("FN"),
    prop = ifelse(total_confusion > 0, value / total_confusion, 0),
    alluvium_id = paste(L1, L3, sep = "⟂")  # stable flow id (TP→confirmed etc.)
  )

# Aesthetics — tweak these as you like
col_confusion <- c(
  "TP"="lightsteelblue","FP"="lightsteelblue2","TN"="rosybrown","FN"="rosybrown2"
)
col_validation <- c(
  "Validated elsewhere"="sandybrown","Not validated"="thistle3"
)
col_subcats <- c(
  "confirmed_links"="coral3","locally_unique_links"="thistle3",
  "cryptic_links"="coral","unsupported_links"="thistle1",
  "possibly_missing_links"="coral2","spurious_links"="thistle",
  "feasible_links"="coral1","likely_forbidden"="thistle2"
)
stratum_fill <- c(col_confusion, col_validation, col_subcats)

flow_alpha <- 0.7     # flow transparency
flow_colour <- NA      # outline color for flows; e.g., "grey30" to draw borders
stratum_label_size <- 4

# Choose whether to plot mean counts or proportions:
metric <- "prop"  # set to "value" for raw mean counts

# Prepare "lodes" format for 3 axes and KEEP alluvium_id
flows_long <- flows %>%
  select(L1, L2, L3, value, prop, alluvium_id) %>%
  pivot_longer(cols = c(L1, L2, L3),
               names_to = "axis", values_to = "stratum") %>%
  mutate(
    axis = recode(axis, L1="Confusion", L2="Validation", L3="Subtype"),
    axis = factor(axis, levels = c("Confusion","Validation","Subtype"))
  )

# Pretty labels for strata
stratum_labeller <- function(x) {
  x %>% str_replace_all("_", " ") %>% str_to_sentence()
}



## -------- choose your orders here --------
order_confusion  <- c("TP", "FP", "TN", "FN")                 # LEFT column order
order_validation <- c("Not validated", "Validated elsewhere") # MIDDLE column order
order_subtypes   <- c(                                       # RIGHT column order
  "locally_unique_links", "spurious_links",
  "likely_forbidden", "unsupported_links",
  "confirmed_links", "possibly_missing_links",
  "feasible_links", "cryptic_links"
)

# Rebuild flows_long with your custom orders applied
flows_long <- flows %>%
  select(L1, L2, L3, value, prop, alluvium_id) %>%
  tidyr::pivot_longer(c(L1, L2, L3), names_to = "axis", values_to = "stratum") %>%
  dplyr::mutate(
    axis = dplyr::recode(axis, L1 = "Confusion", L2 = "Validation", L3 = "Subtype"),
    # make stratum a factor with the order you chose, depending on axis
    stratum = dplyr::case_when(
      axis == "Confusion"  ~ factor(stratum, levels = order_confusion),
      axis == "Validation" ~ factor(stratum, levels = order_validation),
      axis == "Subtype"    ~ factor(stratum, levels = order_subtypes),
      TRUE ~ factor(stratum)
    ),
    # also lock the axis order (left -> middle -> right)
    axis = factor(axis, levels = c("Confusion", "Validation", "Subtype"))
  )

# Plot
# gg <- ggplot(
#   flows_long,
#   aes(x = axis,
#       stratum = stratum,
#       alluvium = alluvium_id,
#       y = .data[[metric]],
#       fill = stratum,
#       label = stratum_labeller(stratum))
# ) +
#   geom_alluvium(color = flow_colour, alpha = flow_alpha, width = 0.25) +
#   geom_stratum(width = 0.03, color = "white") +
#   geom_text(stat = "stratum", size = stratum_label_size, color = "white") +
#   scale_fill_manual(values = stratum_fill, guide = "none") +
#   scale_y_continuous(labels = if (metric=="prop") percent_format(accuracy = 1) else label_number_si()) +
#   labs(
#     title = if (metric=="prop") "Alluvial of mean proportions across iterations"
#     else "Alluvial of mean counts across iterations",
#     subtitle = "Confusion classes → Validation group → Subcategories",
#     x = NULL, y = if (metric=="prop") "Proportion of interactions" else "Mean count"
#   ) +
#   theme_minimal(base_size = 12) +
#   theme(
#     panel.grid.major.x = element_blank(),
#     panel.grid.minor = element_blank(),
#     axis.text.x = element_text(size = 12, face = "bold"),
#     plot.title = element_text(face = "bold"),
#     plot.subtitle = element_text(color = "grey30")
#   )
# 
# gg + tme

# style it:
gg <- ggplot(
  flows_long,
  aes(x = axis,
      stratum = stratum,
      alluvium = alluvium_id,
      y = .data[[metric]],
      fill = stratum)
) +
  geom_stratum(width = 0.03, color = "white") +
  geom_alluvium(color = flow_colour, alpha = flow_alpha, width = 0.25) +
  scale_fill_manual(values = stratum_fill, guide = "none") +
  scale_y_continuous(labels = if (metric=="prop") percent_format(accuracy = 1) else label_number_si()) +
  labs(
    title = if (metric=="prop") "Alluvial of mean proportions across iterations"
    else "Alluvial of mean counts across iterations",
    subtitle = "Confusion classes → Validation group → Subcategories",
    x = NULL, y = if (metric=="prop") "Proportion of interactions" else "Mean count"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.x = element_text(size = 12, face = "bold"),
    plot.title = element_text(face = "bold"),
    plot.subtitle = element_text(color = "grey30")
  )

bg_col <- "white"   # or "#F7F4EF" or whatever your panel background is

gg <- ggplot(
  flows_long,
  aes(x = axis,
      stratum = stratum,
      alluvium = alluvium_id,
      y = .data[[metric]],
      fill = stratum)
) +
  # 2) Wide "mask" strata: same width as flows, fill = background,
  #    so they trim the flows exactly at the axis
  geom_stratum(
    width  = 0.02,
    color  = NA,
    fill   = bg_col,
    alpha  = 1
  ) +
  
  # 3) Narrow visible strata on top
  geom_stratum(
    width = 0.03,
    color = "white"
  ) +
  # 1) Flows first
  geom_alluvium(
    color = flow_colour,
    alpha = flow_alpha,
    width = 0.15,
    knot.pos = 0.2
  ) +
  
  scale_fill_manual(values = stratum_fill, guide = "none") +
  scale_y_continuous(
    labels = if (metric == "prop") percent_format(accuracy = 1)
    else label_number_si()
  ) +
  labs(
    title    = if (metric=="prop") "Alluvial of mean proportions across iterations"
    else "Alluvial of mean counts across iterations",
    subtitle = "Confusion classes → Validation group → Subcategories",
    x = NULL,
    y = if (metric=="prop") "Proportion of interactions" else "Mean count"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor   = element_blank(),
    axis.text.x        = element_text(size = 12, face = "bold"),
    plot.title         = element_text(face = "bold"),
    plot.subtitle      = element_text(color = "grey30")
  )


# totals per stratum (for % text)
stratum_totals <- flows_long %>%
  dplyr::group_by(axis, stratum) %>%
  dplyr::summarise(val = sum(.data[[metric]]), .groups = "drop") %>%
  dplyr::mutate(stratum_chr = as.character(stratum))

# build one hidden stratum layer to grab box geometry (same width as above: 0.25)
tmp_build <- ggplot_build(
  ggplot(flows_long,
         aes(x = axis, stratum = stratum, alluvium = alluvium_id, y = .data[[metric]])) +
    geom_stratum(width = 0.03, color = NA)
)

geo <- as.data.frame(tmp_build$data[[1]])
ax_levels <- levels(flows_long$axis)

# midpoints and axis name for each stratum box
label_geom <- geo %>%
  dplyr::transmute(
    x_mid       = (xmin + xmax)/2,
    y_mid       = (ymin + ymax)/2,
    stratum_chr = as.character(stratum),
    axis        = ax_levels[pmax(1, pmin(length(ax_levels), round((xmin + xmax)/2)))]
  )

# join values + geometry, build the two-line label strings
label_df <- dplyr::left_join(
  stratum_totals,
  label_geom,
  by = c("axis","stratum_chr")
) %>%
  dplyr::mutate(
    name_txt = stratum_chr %>% stringr::str_replace_all("_"," ") %>% stringr::str_to_sentence(),
    pct_txt  = if (metric == "prop") scales::percent(val, accuracy = 1)
    else scales::label_number_si()(val)
  )

# tweakable label settings
label_nudge_x   <- 0.03                                 # push labels to the right of each box
label_nudge_y   <- 0.05  
y_off           <- 0.5 * diff(range(flows_long[[metric]], na.rm = TRUE))  # vertical gap for % line
name_size       <- 4.2
pct_size        <- 3.6
font_family     <- ""                                     # "" = default device font
label_color_map <- c("Confusion"="mistyrose4","Validation"="mistyrose4","Subtype"="mistyrose4")

label_df <- label_df %>%
  mutate(
    label_final = paste0(name_txt, " (", pct_txt, ")")
  )

gg <- gg +
  geom_text(
    data = label_df,
    inherit.aes = FALSE,
    aes(x = x_mid + label_nudge_x, y = y_mid, label = label_final, color = axis),
    fontface = "bold",
    size = name_size,
    family = font_family,
    hjust = 0
  ) +
  scale_color_manual(values = label_color_map, guide = "none") +
  coord_cartesian(clip = "off")

# gg <- gg +
#   # bold category name
#   geom_text(
#     data = label_df,
#     inherit.aes = FALSE,
#     aes(x = x_mid + label_nudge_x, y = y_mid, label = name_txt, color = axis),
#     fontface = "bold", size = name_size, family = font_family, hjust = 0
#   ) +
#   # percentage beneath
#   geom_text(
#     data = label_df,
#     inherit.aes = FALSE,
#     aes(x = x_mid + label_nudge_x, y = y_mid - y_off, label = pct_txt, color = axis),
#     size = pct_size, family = font_family, hjust = 0
#   ) +
#   scale_color_manual(values = label_color_map, guide = "none") +
#   coord_cartesian(clip = "off") +
#   theme(plot.margin = margin(20, 20, 20, 20))  # extra right margin for labels
# 
# gg + tme
gg <- ggplot(
  flows_long,
  aes(x = axis,
      stratum = stratum,
      alluvium = alluvium_id,
      y = .data[[metric]],
      fill = stratum)
) +
  # 2) Wide "mask" strata: same width as flows, fill = background,
  #    so they trim the flows exactly at the axis
  geom_stratum(
    width  = 0.02,
    color  = NA,
    fill   = bg_col,
    alpha  = 1
  ) +
  
  # 3) Narrow visible strata on top
  geom_stratum(
    width = 0.03,
    color = "white"
  ) +
  
  # 1) Flows
  geom_alluvium(
    color    = flow_colour,
    alpha    = flow_alpha,
    width    = 0.15,
    knot.pos = 0.2
  ) +
  
  scale_fill_manual(values = stratum_fill, guide = "none") +
  scale_y_continuous(
    labels = if (metric == "prop") percent_format(accuracy = 1)
    else label_number_si()
  ) +
  labs(
    title    = NULL,   # remove title
    subtitle = NULL,   # remove subtitle
    x = NULL,
    y = if (metric == "prop") "Proportion of interactions" else "Mean count"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor   = element_blank(),
    axis.text.x        = element_text(size = 12, face = "bold"),
    plot.title         = element_text(face = "bold"),
    plot.subtitle      = element_text(color = "grey30")
  )

# range for vertical offsets
yr <- diff(range(flows_long[[metric]], na.rm = TRUE))

# total value per stratum per axis
stratum_totals <- flows_long %>%
  dplyr::group_by(axis, stratum) %>%
  dplyr::summarise(val = sum(.data[[metric]]), .groups = "drop") %>%
  dplyr::mutate(stratum_chr = as.character(stratum))

# build a hidden stratum layer to get box geometry
tmp_build <- ggplot_build(
  ggplot(flows_long,
         aes(x = axis,
             stratum = stratum,
             alluvium = alluvium_id,
             y = .data[[metric]])) +
    geom_stratum(width = 0.03, color = NA)
)

geo <- as.data.frame(tmp_build$data[[1]])
ax_levels <- levels(flows_long$axis)

# midpoints and axis name for each stratum box
label_geom <- geo %>%
  dplyr::transmute(
    x_mid       = (xmin + xmax)/2,
    y_mid       = (ymin + ymax)/2,
    stratum_chr = as.character(stratum),
    axis        = ax_levels[round(x)]  # map numeric x back to axis factor
  )

# join values + geometry, build label text and vertically stagger labels
label_df <- dplyr::left_join(
  stratum_totals,
  label_geom,
  by = c("axis", "stratum_chr")
) %>%
  dplyr::mutate(
    name_txt = stratum_chr %>%
      stringr::str_replace_all("_", " ") %>%
      stringr::str_to_sentence(),
    pct_txt  = if (metric == "prop") scales::percent(val, accuracy = 1)
    else scales::label_number_si()(val),
    label_final = paste0(name_txt, " (", pct_txt, ")")
  ) %>%
  dplyr::arrange(axis, y_mid) %>%
  dplyr::group_by(axis) %>%
  dplyr::mutate(
    # stagger labels within each axis to reduce overlap
    label_y = y_mid + (row_number() - mean(row_number())) * (0.033 * yr)
  ) %>%
  dplyr::ungroup()

label_nudge_x <- 0.03   # adjust if you want labels further right
label_nudge_y <- 0.02

gg <- gg +
  geom_text(
    data = label_df,
    inherit.aes = FALSE,
    aes(x = x_mid + label_nudge_x,
        y = label_y + label_nudge_y,
        label = label_final,
        color = axis),
    fontface = "bold",
    size     = name_size,
    family   = font_family,
    hjust    = 0
  ) +
  scale_color_manual(values = label_color_map, guide = "none") +
  coord_cartesian(clip = "off") +
  theme(
    plot.margin = margin(20, 20, 20, 20)   # room on the right for labels
  )

gg

gg <- gg +
  theme_minimal(base_size = 16) +
  theme(
    panel.grid = element_blank(),
    axis.text = element_blank(),     # remove tick labels
    axis.ticks = element_blank(),    # remove tick marks
    axis.title = element_blank(),    # remove axis titles
    axis.line = element_blank(),     # remove axis lines
    plot.title = element_text(face = "bold"),
    plot.subtitle = element_text(color = "grey30"),
    plot.margin = margin(20, 20, 20, 20)  # keep right margin for labels
  )

gg
# gg <- gg + labs(title = NULL, subtitle = NULL)
# gg

pdf( file = "alluvial_canary.pdf", 
     width = 12, # inches 
     height = 7, 
     family = "Helvetica") # or another installed font
print(gg)
dev.off()

png(
  filename = "alluvial_canary.png",
  width    = 12,
  height   = 7,
  units    = "in",
  res      = 300,
  family   = "Helvetica"
)

# --- your plotting code here ---
print(gg)

dev.off()

# Optional: hide all category labels
show_labels <- FALSE   # TRUE = show; FALSE = remove all

if (show_labels) {
  gg <- gg +
    geom_text(
      data = label_df,
      inherit.aes = FALSE,
      aes(x = x_mid + label_nudge_x,
          y = y_mid,
          label = label_final,
          color = axis),
      fontface = "bold",
      size = name_size,
      family = font_family,
      hjust = 0
    )
}

# ---- certainty in missing links analysis ----
# Calculate per-link statistics across iterations and islands

link_stats <- df_removed %>%
  group_by(node_from, node_to, test_layer) %>%
  summarise(
    obs = max(original_binary),
    pred_freq = mean(predicted_bin_sigm, na.rm = TRUE),
    pred_prob_mean = mean(predicted_prob_sigm, na.rm = TRUE),
    n_iter = n(),
    .groups = "drop"
  )

# Add external support info
# Compute how many other islands each link is actually observed in:

link_presence <- link_stats %>%
  group_by(node_from, node_to) %>%
  summarise(
    n_islands_obs = sum(obs),
    n_islands_total = n(),
    .groups = "drop"
  )

link_stats2 <- link_stats %>%
  left_join(link_presence, by = c("node_from", "node_to")) %>%
  mutate(
    obs_elsewhere = (n_islands_obs > 0 & obs == 0),
    ext_support = if_else(n_islands_total > 1,
                          (n_islands_obs - obs) / (n_islands_total - 1),
                          0)
  )

# Compute a posterior-like missingness probability
# 
# We’ll use a logistic-style function where external support has stronger weight:
#   
# P(missing link is real)=1−exp(−k×(α×f+β×s))
# 
# where:
#   
#   𝑓
# f = prediction frequency,
# 
# 𝑠
# s = external support,
# 
# 𝛼
# α and 
# 𝛽
# β are weights (e.g. 1 and 3 if you want to emphasize external support),
# 
# 𝑘
# k = scaling factor controlling curve steepness.

alpha <- 1    # weight for prediction frequency
beta  <- 3    # weight for external support (heavier)
k     <- 4    # overall steepness

missing_links <- link_stats2 %>%
  filter(obs == 0, obs_elsewhere) %>%
  mutate(
    score = alpha * pred_freq + beta * ext_support,
    missing_prob = 1 - exp(-k * score)
  ) %>%
  arrange(desc(missing_prob))

# (Optional) Visualize the relationship


ggplot(missing_links, aes(x = pred_freq, y = missing_prob, color = ext_support)) +
  geom_point(alpha = 0.7) +
  scale_color_viridis_c(option = "plasma", end = 0.9) +
  geom_smooth(method = "loess", se = FALSE, color = "grey30") +
  labs(
    x = "Prediction frequency across iterations",
    y = "Estimated probability of true missing link",
    color = "External support\n(proportion of islands)",
    title = "Evidence-weighted probability of true missing links",
    subtitle = "Combining frequency of prediction and number of external islands"
  ) +
  theme_minimal(base_size = 12) + tme

